---
tags:
  - ensemble
  - bagging
  - bootstrap
  - boosting
  - aggregration
  - categorical-data
  - continous-data
  - random-forest
  - decision-tree
  - AdaBoost
  - bias
  - variance
sticker: lucide//plus
---
**cf**[^0]

: ì•™ìƒë¸” ê¸°ë²•(Ensemble Learning)ì€ ==ì—¬ëŸ¬ ê°œì˜ ê°œë³„ ëª¨ë¸ì„ ì¡°í•©í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ë¡œ ì¼ë°˜í™”==í•˜ëŠ” ë°©ë²•

- ì—¬ëŸ¬ ê°œì˜ weak classifier[^1]ë¥¼ ê²°í•©í•˜ì—¬ strong classifier[^2]ë¥¼ ë§Œë“œëŠ” ê²ƒ
	ex) ì£¼ë¡œ ì—¬ëŸ¬ ê°œì˜ [[Decision Trees]] ë¥¼ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ ê²°ì • íŠ¸ë¦¬ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•
- Ensemble Learningì—ì„œëŠ” **Bagging**ê³¼ **Boosting**ì´ ìˆìŒ

##### Bias and Variance
- **Bias**: (unknown) true functionê³¼ hypothesis(or model) _h(x)_ ë“¤ì˜ í‰ê·  ì˜ˆì¸¡ì˜ ì°¨ì´
- **Variance**: hypothesisê°€ training setì— ë”°ë¼ ë™ì¼í•œ ì…ë ¥ xì— ëŒ€í•´ ë³€í™”í•˜ëŠ” ì •ë„(training setì´ ë‹¬ë¼ì§€ë©´ì„œ xì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì˜ ë³€í™”ëŸ‰)
	![[Pasted image 20231025111820.png]]
	- biasê°€ í¬ë©´ model ë‹¨ìˆœí•´ì§€ê³  ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” underfittingì´ ë°œìƒ
	- varianceê°€ í¬ë©´ modelì´ ë³µì¡í•´ì§€ê³  overfittingì´ ë°œìƒ

##### Bayes Optimality
- íŠ¹ì • ìƒí™©ì—ì„œ ê°€ì¥ ì¢‹ì€ ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦´ ë•Œ, ë² ì´ì§€ì•ˆ ì¶”ë¡ ê³¼ í™•ë¥ ë¡ ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í•œ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê²ƒì„ ì˜ë¯¸
	![[Pasted image 20231025110556.png]]
	- ìœ„ ì‹ì˜ ìµœì†Œê°’ì¼ë•Œ yê°’ì„ êµ¬í•˜ëŠ”ê²ƒì´ ëª©í‘œ
		- ì²« ë²ˆì§¸ í•­ì€ ìŒìˆ˜ê°€ ì•„ë‹ˆë©°, _ğ‘¦ = ğ‘¦âˆ—_ ë¡œ ì„¤ì •í•˜ì—¬ 0ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ
		- ë‘ ë²ˆì§¸ í•­ì€ ëŒ€ìƒì˜ **inherent unprdictability**(ì˜ˆì¸¡í•  ìˆ˜ ì—†ëŠ” ë¶ˆí™•ì‹¤ì„±), or **noise**, of the targets, and is called the **Bayes error**  
	![[Pasted image 20231025111311.png]]
	![[Pasted image 20231025111405.png]]
	- **bias**: how wrong the expected prediction is
	- **variance**: ì˜ˆì¸¡ì˜ ë³€ë™ì„± ì •ë„
	- **Bayes error**: ê°€ëŠ¥í•œ ìµœì†Œì˜ ì˜¤ë¥˜, ì˜ˆì¸¡í•  ìˆ˜ ì—†ëŠ” ë¶ˆí™•ì‹¤ì„±, í†µì œ ë²”ìœ„ ë²—ì–´ë‚¨
	![[Pasted image 20231025111909.png]]


- **Bayes Optimal Classifier**: ì£¼ì–´ì§„ train ë°ì´í„°ì…‹ì„ ë°”íƒ•ìœ¼ë¡œ, ì „ì— ë³´ì§€ ëª»í•œ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê°€ì¥ ê·¸ëŸ´ ë“¯í•œ ì˜ˆì¸¡ì„ ê³„ì‚°í•´ì£¼ëŠ” í™•ë¥  ëª¨ë¸
	- P(A|B): **ì‚¬í›„í™•ë¥ (posterior)**, ì‚¬ê±´ Bê°€ ë°œìƒí•œ í›„ ê°±ì‹ ëœ ì‚¬ê±´ Aì˜ í™•ë¥ , ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê³ ë ¤í•œ í›„ì— ì–´ë–¤ ê²°ì •ì´ ê°€ì¥ ê°€ëŠ¥ì„± ìˆëŠ”ì§€ ë‚˜íƒ€ëƒ„
	- P(A): **ì‚¬ì „í™•ë¥ (prior)**, ì‚¬ê±´ Bê°€ ë°œìƒí•˜ê¸° ì „ì— ê°€ì§€ê³  ìˆë˜ ì‚¬ê±´ Aì˜ í™•ë¥ , ê° ê²°ì •ì´ ì–¼ë§ˆë‚˜ ê°€ëŠ¥ì„±ì´ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„



### Bagging

- **Bootstrap Aggregation**
- Train classifiers **independently** on **random** subsets of the training data
	- sampleì„ ì—¬ëŸ¬ ë²ˆ ë½‘ì•„(Bootstrap, ==ì „ì²´ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ëœë¤í•˜ê²Œ ë³µì›ì¶”ì¶œì„ ì§„í–‰==) ê° ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ(==ê°œë³„ì ì¸ ëª¨ë¸ì€ ë…ë¦½ì ==) ê²°ê³¼ë¬¼ì„ ì§‘ê³„(Aggregration)í•˜ëŠ” ê¸°ë²•
- varianceì„ ì¤„ì—¬ì£¼ëŠ” íš¨ê³¼
	- biasë¥¼ ì¤„ì—¬ì£¼ì§€ ì•ŠìŒ 
- Boostingê³¼ ë‹¤ë¥´ê²Œ **ê° ëª¨ë¸ë“¤ì€ ì„œë¡œ ë…ë¦½ì **ì´ê³  **ë³‘ë ¬ë¡œ í•™ìŠµ**
	![[Pasted image 20231013150857.png]]
	- **Categorical Data**: íˆ¬í‘œ ë°©ì‹(voting)ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì§‘ê³„, ì „ì²´ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡í•œ ê°’ ì¤‘ ê°€ì¥ ë§ì€ ê°’ì„ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ ì •
	- **Continuous Data**: í‰ê· ìœ¼ë¡œ ì§‘ê³„

- ë¬¸ì œì : the datasets are not independent, so we don't get the _1/m_ variance reduction
	![[Pasted image 20231025113118.png]]
	- ìœ„ì— ì‹ì€ sampled predictionì„ ë‚˜íƒ€ë‚¸ê²ƒ
	- _ğœŒ_: dataë“¤ì˜ ìƒê´€ê´€ê³„, Bayes error ì²˜ëŸ¼ í†µì œ x
	- data setì„ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ sampling í•˜ëŠ” ê²ƒì„ ì–´ë ¤ìš°ë¯€ë¡œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì˜ ë…ë¦½ì„±ì„ ë†’ì„, ì¦‰ ìœ„ì˜ ì‹ì—ì„œ ğœŒë¥¼ ì¤„ì„

##### Random Forest
- [[Decision Trees]] ê°€ ëª¨ì—¬ Random Forestë¥¼ êµ¬ì„±, ì´ëŠ” overfittingì˜ ë¬¸ì œì ì„ í•´ê²°
	- ìˆ˜ë§ì€ Featureë¥¼ ê¸°ë°˜ìœ¼ë¡œ labelë¥¼ ì˜ˆì¸¡í•˜ë©´ ë†’ì€ í™•ë¥ ë¡œ overfittingì´ ë°œìƒ
	- ì—¬ëŸ¬ ê°œì˜ ì‘ì€ Decision Treesê°€ ì˜ˆì¸¡í•œ ê°’ë“¤ ì¤‘ ê°€ì¥ ë§ì€ ê°’(ë¶„ë¥˜) or í‰ê· ê°’(íšŒê·€)ë¥¼ ìµœì¢… ì˜ˆì¸¡ ê°’ìœ¼ë¡œ ì •í•¨
	![[Pasted image 20231013151924.png]]
	- **n_estimators**: random forest ì•ˆì˜ ê²°ì • íŠ¸ë¦¬ ê°¯ìˆ˜
		- í´ìˆ˜ë¡ ì¢‹ì§€ë§Œ ê·¸ë§Œí¼ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ì´ ì¦ê°€
	- **max_feature**: ë¬´ì‘ìœ„ë¡œ ì„ íƒí•  Feature ê°œìˆ˜
		- _bootstrap = True_ ì¸ ê²½ìš° ì „ì²´ Featureì—ì„œ ë³µì› ì¶”ì¶œí•´ì„œ íŠ¸ë¦¬ë¥¼ ë§Œë“¬

---

### Boosting

- ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•˜ì—¬ weak classifierë¥¼ strong classifierë¡œ ë§Œë“œëŠ” ë°©ë²•
- biasì„ ì¤„ì—¬ì£¼ëŠ” íš¨ê³¼
	- varianceëŠ” ë³´ì¥ì„ ëª»í•¨
- Baggingê³¼ ë‹¤ë¥´ê²Œ **ê°ê°ì˜ ëª¨ë¸ì˜ ì—°ê´€ì„±**ì´ ìˆê³  **ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ**
	- ìˆœì°¨ì ìœ¼ë¡œ ì• ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ì— ë”°ë¼ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ê°€ ë¶€ì—¬ë˜ê³ , ë¶€ì—¬ëœ ê°€ì¤‘ì¹˜ê°€ ë‹¤ìŒ ëª¨ë¸ì— ì˜í–¥ì„ ì¤Œ
	- ì˜ëª» ë¶„ë¥˜ëœ ë°ì´í„°ì— ì§‘ì¤‘í•˜ì—¬ ìƒˆë¡œìš´ ë¶„ë¥˜ ê·œì¹™ì„ ë§Œë“œëŠ” ë‹¨ê³„ë¥¼ ë°˜ë³µ
	- ê³ ì •ëœ í•™ìŠµ ë°ì´í„°
	![[Pasted image 20231013153204.png]]
	- D2ë¥¼ ë³´ë©´ D1ì—ì„œ ì˜ ë¶„ë¥˜ëœ ë°ì´í„°ëŠ” í¬ê¸°ê°€ ì‘ì•„ì§(ê°€ì¤‘ì¹˜ê°€ ì‘ì•„ì§), ì˜ëª» ë¶„ë¥˜ëœ ë°ì´í„°ëŠ” í¬ê¸°ê°€ ì»¤ì§(ê°€ì¤‘ì¹˜ê°€ ì»¤ì§)

- Baggingì— ë¹„í•´ errorê°€ ì ì–´ ì„±ëŠ¥ì´ ì¢‹ìŒ
- ì†ë„ê°€ ëŠë¦¬ê³  overfitting ë  ê°€ëŠ¥ì„±ì´ ìˆìŒ
	- ì¦‰, ê°œë³„ ê²°ì • íŠ¸ë¦¬ì˜ ë‚®ì€ ì„±ëŠ¥ì´ ë¬¸ì œì¸ ê²½ìš° Boostingì´ ì í•©
	- overfittingì´ ë¬¸ì œì¸ ê²½ìš° Baggingì´ ì í•©

##### Weak Classifier
- ì•½ ë¶„ë¥˜ê¸°, ë‹¨ì¼ ë¶„ë¥˜ ê·œì¹™ ë˜ëŠ” ëª¨ë¸ë¡œ êµ¬ì„±
- ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì„±ëŠ¥, ë†’ì€ ì •í™•ë„ë¥¼ ê¸°ëŒ€í•˜ê¸° í˜ë“¬
	![[Pasted image 20231026142453.png]]
	- ìœ„ ì‚¬ì§„ì€ weak classifier
	- horizontal and vertiacl half spaceë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” decision stumps
	![[Pasted image 20231026142649.png]]
	- ìµœëŒ€ _1/2 âˆ’ Î³_ ì˜ ê°’
	- err: í‹€ë¦° ê²½ìš°ë¥¼ 1ë¡œ ë³´ê³ (indicator function) í‹€ë¦° ê²½ìš°ì— _x Wi_ ë¥¼ í•´ì¤Œ


##### AdaBoost
- ê°ê°ì˜ ë¶„ë¥˜ê¸°ëŠ” ì´ì „ì˜ mistakeì— ì§‘ì¤‘í•´ ==biasë¥¼ ì¤„ì—¬ì¤Œ== 
- AdaBoostëŠ” ì—¬ëŸ¬ê°œì˜ stumpë¡œ êµ¬ì„±
	- **stump**: í•˜ë‚˜ì˜ nodeì— 2ê°œì˜ leafë¥¼ ê°€ì§„ tree
	- íŠ¸ë¦¬ì™€ ë‹¤ë¥´ê²Œ stumpëŠ” ì •í™•í•œ ë¶„ë¥˜ë¥¼ í•˜ì§€ ëª»í•¨
	- stumpëŠ” ë‹¨ í•˜ë‚˜ì˜ ì§ˆë¬¸ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•´ì•¼ í•˜ë¯€ë¡œ weak learner

- Key steps
	1. At each iteration we re-weight the training samples by assigning larger weights to samples that were classified incorrectly
	2. Train a new weak classifier based on the re-weighted samples
	3. Add this weak classifier to the ensemble of classifiers(new classifier)
	4. Weight each weak classifier in the ensemble with somw weights
	5. repeat the process many times
	![[111.jpg]]
	![[222.jpg]]
	![[333.jpg]]
	![[444.jpg]]
	![[555.jpg]]
	![[Pasted image 20231026152914.png]]

- AdaBoost Minimize the Training Error
	![[Pasted image 20231026153254.png]]


- ê°ê°ì˜ íŠ¸ë¦¬ê°€ ë™ë“±í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ëŠ” Random Forestì™€ ë‹¬ë¦¬ AdaBoostì—ì„œëŠ” íŠ¹ì • stumpê°€ ë‹¤ë¥¸ stumpë³´ë‹¤ ë” ì¤‘ìš”
	![[Pasted image 20231013201319.png]]
	- í¬ê¸°ê°€ í° ê²ƒì€ ê°€ì¤‘ì¹˜ê°€ ë” ë†’ì€ stumpë¥¼ ëœ»í•¨
	- ê°€ì¤‘ì¹˜ê°€ ë†’ë‹¤ëŠ” ê²ƒì€ **Amount of Say**ê°€ ë†’ë‹¤ê³  í‘œí˜„, ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í¬ë‹¤ëŠ” ëœ»

AdaBoostì˜ íŠ¹ì§•
	1. weak learnerë¡œ êµ¬ì„±, weak learnerì€ stumpì˜ í˜•íƒœ
	2. ì–´ë–¤ stumpëŠ” ë‹¤ë¥¸ stumpë³´ë‹¤ ê°€ì¤‘ì¹˜ê°€ ë†’ìŒ
	3. ê° stumpì˜ errorëŠ” ë‹¤ìŒ stumpì˜ ê²°ê³¼ì— ì˜í–¥ì„ ì¤Œ







[^0]: https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-11-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning-%EB%B0%B0%EA%B9%85Bagging%EA%B3%BC-%EB%B6%80%EC%8A%A4%ED%8C%85Boosting
	- https://medium.com/dawn-cau/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-%EC%9D%B4%EB%9E%80-cf1fcb97f9d0
	- https://daje0601.tistory.com/120
	- https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost

[^1]: weak classifier
	- ì•½ ë¶„ë¥˜ê¸°
	- ë‹¨ì¼ ë¶„ë¥˜ ê·œì¹™ ë˜ëŠ” ëª¨ë¸ë¡œ êµ¬ì„±
	- ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì„±ëŠ¥, ë†’ì€ ì •í™•ë„ë¥¼ ê¸°ëŒ€í•˜ê¸° í˜ë“¬

[^2]: strong classifier
	- ê°•í•œ ë¶„ë¥˜ê¸°
	- ì—¬ëŸ¬ ì•½í•œ ë¶„ë¥˜ê¸°ì˜ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±
	- ì´ë¥¼ ìœ„í•´ ì¼ë°˜ì ìœ¼ë¡œ ensemble í•™ìŠµ ê¸°ìˆ ì„ ì‚¬ìš© 
		- AdaBoost
		- Random Forest