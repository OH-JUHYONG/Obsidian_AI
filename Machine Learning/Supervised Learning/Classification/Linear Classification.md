---
tags:
  - linear-classification
  - classification
  - binary-classification
  - threshold
  - dummy-feature
  - feasible-region
  - 0-1-loss
  - loss-function
  - GD
  - SGD
  - stochastic-gradient-descent
  - surrogate-loss-function
  - linear-regression
  - sigmoid
  - sigmoidal
  - log-linear
  - perceptron
  - cross-entropy
  - logistic-regression
  - decision-boundary
  - multiclass-classification
  - one-of-K
  - one-hot-vector
  - softmax
  - softmax-regression
  - multiclass-logistic-regression
  - gradient-checking
  - learning-rate
  - oscillation
  - training-curve
  - mini-batch
  - convex
  - convex-sets
  - convexity
  - XOR-function
---
: ì¼ì°¨ì› í˜¹ì€ ë‹¤ì°¨ì› ë°ì´í„°ë“¤ì„ linear modelì„ ì´ìš©í•˜ì—¬ í´ë˜ìŠ¤ë“¤ë¡œ ë¶„ë¥˜


- **Binary classification**: predicting a binary-valued target
	- ì„ í˜• ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ë©´ ì§ì„ ì´ ê·¸ë ¤ì§
	- ì„ í˜• ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ì§€ ì•Šë‹¤ë©´ ì§ì„ ìœ¼ë¡œ ê·¸ë ¤ì§€ì§€ ì•ŠìŒ

### Binary linear classifiers

- **Classification**: predicting a discrete-value target
- **binary**: predict a binary target _t âˆˆ {0, 1}_ / postivie & negative
- **linear**: model is a linear function of _x_, followed by a **threshold**(ê¸°ì¤€ê°’)
	![[Pasted image 20231028104606.png]]
	- _w_: weight vector
	- _b_: scalar-valued bias

- **training set** consist of a set of _N_ pairs _(x^(i), t^(i))_ 
	- _x^(i)_: input
	- _t^(i)_: the binary-valued target or label 

#### goal
- correctly classify all the training cases
- compute a linear function of the inputs, and determine whether or not the value is larger than some **threshold** _r_ 
- **Eliminating the threshold**
	![[Pasted image 20231028104724.png]]
	![[Pasted image 20231028104930.png]]
	-  biasë¥¼ _b-r_, _r = 0_ ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¼ë°˜ì„±ì„ ìƒì§€ ì•Šê²Œ í•¨

- **Eliminating the bias**
	![[Pasted image 20231028110616.png]]  
	![[Pasted image 20231028110636.png]]
	- biasë¥¼ ì‚­ì œí•˜ê³  í•­ìƒ 1ì˜ ê°’ì„ ì·¨í•˜ëŠ” **dummy feature** ì¸ _x0_ ë¥¼ ì¶”ê°€, ì´ëŠ” íš¨ê³¼ì ìœ¼ë¡œ biasì˜ ì—­í• ì„ í•¨

#### Exapmles
##### _NOT_
- training set
	![[Pasted image 20231028112231.png]]

##### _AND_
- training set
	![[Pasted image 20231028112136.png]]


#### The geometric picture
##### Data space
- focus on two-dimensional input and weight spaces
- Each point in **data space**(or **input space**) correspond to a possible **input vector**
	- postive region / negative region <-- ì´ ë‘ ì§€ì—­ì„ ë‚˜ëˆ„ëŠ” **decision boundary**(wT x + b = 0)

#### _NOT example_
- **Input space & Data space**
	![[Pasted image 20231028120126.png]]
	-  Hypotheses _w_ can be represented by **half-space**
	![[Pasted image 20231028120225.png]]

- **Weight space**
	![[Pasted image 20231028121059.png]]
	- **feasible region**: ëª¨ë“  ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì˜ì—­, ë§Œì•½ ì´ ì˜ì—­ì´ ë¹„ì–´ ìˆì§€ ì•Šìœ¼ë©´ feasibleí•¨


#### Choosing a cost function
- ë°ì´í„° ì„¸íŠ¸ê°€ ì„ í˜•ì ìœ¼ë¡œ ë¶„ë¦¬ë  ìˆ˜ ì—†ëŠ” ê²½ìš° ì–´ë–»ê²Œ í•©ë¦¬ì ì¸ í•™ìŠµ ê¸°ì¤€ì„ ì •í•  ìˆ˜ ìˆëŠ”ì§€
	- ì˜ëª» ë¶„ë¥˜ëœ í›ˆë ¨ ì˜ˆì œì˜ ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒê²ƒ
##### 0-1 loss
- **loss function**      
	![[Pasted image 20231028175836.png]]
	![[Pasted image 20231028175940.png]]
	- **indicator function**
	- the error rate
	 ![[Pasted image 20231028180329.png]]
	![[Pasted image 20231028181730.png]]
	- 0-1 loss functionì€ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì€ ê³„ì‚°ì ìœ¼ë¡œ ì–´ë µê³ , ë™ì¼í•œ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ëŠ” ì—¬ëŸ¬ ê°€ì„¤ì„ êµ¬ë¶„í•  ìˆ˜ ì—†ìŒ, ë˜ **SGD ê¸°ë°˜ optimizationì´ ë¶ˆê°€ëŠ¥**í•˜ê¸° ë•Œë¬¸ì— 0-1 lossë¥¼ ê·¼ì‚¬í•˜ëŠ” ì—°ì†í•¨ìˆ˜ë¥¼ ê³ ë ¤
	- **surrogate loss function**: ì‹ ë¢°í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¥¼ ëœ ì‹ ë¢°í•˜ì§€ë§Œ ìµœì í•˜ê¸° ì‰¬ìš´ ë‹¤ë¥¸ ì†ì‹¤ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•˜ëŠ” ê²ƒ

##### [[Linear Regression]]
- 0-1 lossì˜ surrogate lossë¡œ squared error loss ê³ ë ¤
	![[Pasted image 20231028182424.png]] ^24af15
	- errorì— ëŒ€í•´ì„œ penaltyë¥¼ ë” í¬ê²Œ ì£¼ê¸° ìœ„í•´ | | ì´ ì•„ë‹Œ _1/2( )^2_ ë¡œ ë‘ëŠ” ê²ƒ
	![[Pasted image 20231028182740.png]]
	- class score _y_ ê°€ positiveì´ë©´ penaltyê°€ ì ì–´ì•¼ í•˜ëŠ”ë° ì˜ë„ì™€ ë‹¤ë¥´ê²Œ ì»¤ì§
	- ì •ë‹µì¼ ìˆ˜ë¡ errorê°€ ì»¤ì§€ëŠ” ë¬¸ì œ

##### Logistic Activation Function
- The **logistic function** if a kind of **sigmoidal**
	![[Pasted image 20231028183908.png]]
- A linear model with a logistic nonlinearity is known as **log-linear**  
	![[Pasted image 20231028184004.png]]
	![[Pasted image 20231028184523.png]]![[Pasted image 20231028184316.png]]


##### Logistic regression
- **CE**(cross-entropy): ë‘ í™•ë¥ ë¶„í¬ê°€ ==ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ ì¸¡ì •==, ë‘ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©
	![[Pasted image 20231028204521.png]]
	- ë” ë†’ì€ confidenceë¥¼ ê°€ì§„ ì˜ˆì¸¡ì— ëŒ€í•´ í‹€ë¦° ê²½ìš° ë” ë§ì€ penaltyë¥¼ ë¶€ê³¼

- logistic activation function(sigmoid)ë¥¼ cross-entropy lossì™€ ê²°í•©í•˜ë©´ **logistic regression**
	- sigmoid functionì„ ì‚¬ìš©í•˜ì—¬ yê°’ì´ 0~1 ì‚¬ì´ë¡œ ì œí•œë˜ì–´ linear regressionë³´ë‹¤ ê°’ì˜ ë³€í™”ê°€ ì ì–´ overfittingì— ìœ ë¦¬
	- dataê°€ decision boundaryì—ì„œ ë©€ë¦¬ ë–¨ì–´ì¡Œì„ ê²½ìš° dataëŠ” 0 or 1 ê°’ì— ê°€ê¹Œì´ ê°€ê²Œ ë˜ê³  ì´ë•Œì˜ ê¸°ìš¸ê¸°ëŠ” 0ê³¼ ê°€ê¹Œì›Œ decision boundaryê°€ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ì§€ ì•Šê²Œ ë¨
	![[Pasted image 20231028204840.png]]
	![[Pasted image 20231028204855.png]]

#####  Comparsion of loss function
	![[Pasted image 20231028205057.png]]




### Multiclass Classification
##### targetì„ í‘œí˜„í•˜ëŠ” ë°©ë²•: one-hot vector
- **one-of-K encoding** ì´ë¼ê³  ë¶ˆë¦¼
- ëª¨ë¸ì˜ linear partì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ _K x D_ **weight matrix** ì™€ **_K_-dimensional bias vector** í•„ìš”
	- _K_: output
	- _D_: input  
	![[Pasted image 20231031133052.png]]
	![[Pasted image 20231031133208.png]]

##### Activation function: softmax function
- a multivariate generalization of the logistic function
- **input**: _zk_ are called the **logits**
- **output**: nonneagtiva and **sum to 1**
- _K_ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¡œ í•´ì„ì´ ê°€ëŠ¥(class scoreì„ í™•ë¥ ë¡œ ë³€í™˜) 
	- _K = 2_ ì¸ ê²½ìš° logistic functionê³¼ ê°™ìŒ 
- _zk_ ê°€ ë‹¤ë¥¸ ê°’ë“¤ë³´ë‹¤ ì›”ë“±í•˜ê²Œ í´ ê²½ìš° (expë¥¼ ì·¨í•˜ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ê°’ë“¤ê³¼ ê²©ì°¨ê°€ ì—„ì²­ ë²Œì–´ì§) _zk_ ì— í•´ë‹¹í•˜ëŠ” ê°’ë§Œ 1ì´ ë˜ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì´ ë˜ëŠ” **argmax**ë¡œ ë³¼ ìˆ˜ ìˆìŒ
	![[Pasted image 20231031133609.png]]

##### the loss function: Cross-entropy
- Cross-entropy can be generalized to the multiple-output case
	![[Pasted image 20231031133855.png]]
	-  one-hot vector ì´ë¯€ë¡œ _tk_ ì¤‘ í•˜ë‚˜ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0
		![[Pasted image 20231031140756.png]]

ìœ„ì— ìš”ì†Œë“¤ì„ ëª¨ë‘ í•©ì³ì„œ **multiclass logistic regression**(or **softmax regression**)ì„ í‘œí˜„
	![[Pasted image 20231031134257.png]]

- softmaxì™€ cross-entropy functionëŠ” ì„œë¡œ ì˜ ìƒí˜¸ì‘ìš©í•˜ë¯€ë¡œ ìˆ˜ì¹˜ ì•ˆì „ì„±ì„ ìœ„í•´ í•­ìƒ single softmax-cross-entropy functionì¸ _L_SCE_ ë¡œ ê²°í•©
	![[Pasted image 20231031141407.png]]


### Gradient Checking
- êµ¬í•œ ê¸°ìš¸ê¸°ê°€ ë§ëŠ”ì§€ ì•„ë‹Œì§€ íŒë‹¨í•˜ëŠ” ë°©ë²•
	![[Pasted image 20231031154736.png]]
	- ê¸°ìš¸ê¸°ë¥¼ ì˜ëª»êµ¬í•œ ê²½ìš° ìµœì í™”ì— ì–´ë ¤ì›€ì´ ìˆìŒ, í•™ìŠµì´ ì•ˆë  ìˆ˜ ìˆìŒ 


### Learning Rate
- the learning rate **Î±** is a hyperparameter
	- **Î±** too small: ë„ˆë¬´ ëŠë¦¬ê²Œ í•™ìŠµë¨, ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
	- **Î±** too large: **oscillation**(ìˆ˜ë ´ì´ ì•ˆë¨)
	- **Î±** much too large: í•™ìŠµ ìì²´ê°€ ë¶ˆì•ˆì •í•´ì§
		- ì ë‹¹í•œ ê°’ì€ _0.001 ~ 0.1_ 
	![[Pasted image 20231031155433.png]]


### Training Curve
- To diagnose optimization problem, it's useful to look at training curves
	![[Pasted image 20231031155818.png]]
	- training curve ë§Œìœ¼ë¡œ ìµœì í™”ê°€ ì˜ ì´ë£¨ì–´ì§€ê³  ìˆëŠ”ì§€ ì•„ë‹Œì§€ ë§í•˜ê¸° ì–´ë ¤ì›€, ìƒëŒ€ì ìœ¼ë¡œ ì„¤ì •í•œ parameterë“¤ì„ ì¡°ì •í•´ì•¼ í•¨ì„ íŒë‹¨í•  ìˆ˜ ìˆìŒ

### Stochastic Gradient Descent
- the cost function _J_ 
	![[Pasted image 20231031160540.png]]
	![[Pasted image 20231031161046.png]]
	- ê° dataë“¤ì˜ lossë¥¼ gradientë¥¼ êµ¬í•´ì„œ í‰ê·   --> **batch training**
	- **SGD**: update the parameters based on the gradient for a single training example, chosen **uniformly at random**
		- variance ë³€í™” ì—†ì´ ë°”ë¡œ ì›í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ(ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)ë˜ëŠ” GDì™€ ë‹¬ë¦¬ SGDëŠ” í•™ìŠµì´ ë¹ ë¦„(ëŒ€ì‹ ì— **variance ì»¤ì§**, **noisy ë°œìƒ**)
		- dataê°€ ë¬´ìˆ˜íˆ ë§ì€ ê²½ìš°ì— íš¨ìœ¨
		- SGD is an unbiased estimate of the batch gradient, ê° dataë“¤ì˜ gradientì— ëŒ€í•œ í‰ê· ì„ êµ¬í•˜ë©´ cost functionì— ëŒ€í•œ ë¯¸ë¶„ì´ë‘ ê°™ìŒ 
		![[Pasted image 20231031161645.png]]


#### mini-batch
- GDì™€ SGDì˜ ì¤‘ê°„ì§€ì , í‘œì¤€ í•™ìŠµ ë°©ë²•
	- **mini-batch**ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ SGDì˜ ë°©ì‹ì˜ varianceë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŒ
- compute the gradients on a randomly chosen medium-sized set of training example
	![[Pasted image 20231031162322.png]]
	- the mini-batch size _|M|_ is a hyperparameter
		- Too large: ê³„ì‚°í•˜ëŠ”ë° ì‹œê°„ì´ ë§ì´ ê±¸ë¦¼, ë©”ëª¨ë¦¬ ì»¤ì§
		- Too small: vectorí™”ëœ ì„±ëŠ¥ì„ í™œìš© ëª»í•¨
		- _|M| = 100_ ì´ ì ë‹¹í•œ í¬ê¸°

#### SGD Learning Rate
- ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµ ì´ˆê¸°ì—ëŠ” learning rateì„ í¬ê²Œ í•´ ìˆ˜ë ´ì— ê°€ê¹Œì›Œì§€ë„ë¡ ì„¤ì • í›„ ì ì  learning rateì„ ì ì  ì¤„ì—¬ ì•ˆì •í™” ì‹œí‚¤ëŠ” ì‘ì—…ì„ ì·¨í•¨
	![[Pasted image 20231031163201.png]]
 
### Convex Sets
- **Convex**: ì§‘í•© _S_ ì—ì„œ ì„ì˜ì˜ ë‘ ì ì„ ì—°ê²°í•˜ëŠ” ì„ ë¶„ì´ S ì•ˆì— ìˆìœ¼ë©´ ë‘ ì ì„ ì—°ê²°í•˜ëŠ” ì„ ë¶„ì´ S ì•ˆì— ì™„ì „íˆ ìœ„ì¹˜í•˜ëŠ” ê²½ìš°ë¥¼ ë§í•¨
	![[Pasted image 20231031142641.png]]

##### Convex Function
- ë‹¤ìŒì„ ë§Œì¡±í•˜ëŠ”ê²ƒì´ **convex function**(<--> concave function)
	![[Pasted image 20231031143601.png]]
	-  Convexityê°€ ì¤‘ìš”í•œ ì´ìœ ??
		-  All critical points are minima
		- Gradient descent finds the optimal solution(will always converge to the global minimum)



### Limits of Linear Classification
##### XOR function
- XORì´ linear separable í•˜ì§€ ì•ŠìŒì„ ì¦ëª…í•˜ëŠ” ë°©ë²•(linear decision boundaryë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ)
	- **ì„ í˜•ìœ¼ë¡œ ë¶„ë¦¬ê°€ ëœë‹¤ë©´** decision boundary ê¸°ì¤€ìœ¼ë¡œ data spaceë¥¼ ìª¼ê°œë©´ half spaceëŠ” **convexí•œ ì„±ì§ˆ**ì„ ê°–ì¶°ì•¼ í•¨
	- í•´ë‹¹ ì„±ì§ˆì„ XORì— ëŒ€ì…ì„ í•˜ë©´ êµì°¨ì ì´ ë‘ ê°œì˜ half spaceì— ë†“ì—¬ì ¸ ìˆê¸° ë•Œë¬¸ì— ì„ í˜• ë¶„ë¦¬ê°€ ì•ˆë¨ì„ ì¦ëª…
	![[Pasted image 20231031153637.png]]
- í•´ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 3ì°¨ì›ì„ í™•ì¥, _x1_ **x** _x2_ ê°’ì„ ìƒˆë¡œìš´ featureë¡œ ì¶”ê°€ 
	![[Pasted image 20231031145749.png]]
	- ex)...ì…ë ¥ ë°ì´í„°ë¥¼ ì„ í˜•ìœ¼ë¡œ ë¶„ë¦¬í•  ìˆ˜ ìˆê²Œ ë³€í™˜í•´ ì£¼ëŠ” **ì‹ ê²½ë§**


### Separating Hyperplanes
- ì—¬ëŸ¬ decision boundaryì—ì„œ ì¢‹ì€ decision boundaryëŠ” generalizationì´ ì˜ë˜ëŠ” ê²ƒ
- **hyperplane**: w^Tx + b = 0(decision boundary)ì˜í•´ ê²°ì •ë˜ëŠ” ì§‘í•© 
	![](Pasted%20image%2020231213110013.png)
#### Optimal Seperating Hyperplane
- ë‘ í´ë˜ìŠ¤ë¥¼ ë¶„ë¦¬í•˜ê³  ë‘ í´ë˜ìŠ¤ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì§€ì ê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ëŠ” hyperplane, ë‘ ë¶„ë¥˜ê¸°ì˜ **margin**(ì„ ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì–‘ ì˜† ë°ì´í„°ì™€ì˜ ê±°ë¦¬)ì„ **ìµœëŒ€** 
	![](Pasted%20image%2020231213110640.png)

#### Geometry of Points and Planes
- **signed distance**: Geometryì˜ ë‚´ë¶€ëŠ” positive, ì™¸ë¶€ëŠ” negative, ê·¸ë¦¬ê³  Edgeì— ê·¼ì ‘í•  ìˆ˜ë¡ 0ì´ ë˜ëŠ” í˜•íƒœì˜ ë°ì´í„°
	![](Pasted%20image%2020231213111303.png)![](Pasted%20image%2020231213111410.png)
	![](Pasted%20image%2020231213112726.png)
	![](Pasted%20image%2020231213113157.png)

#### Maximizing Margin as an Optimization Problem
##### SVM
- ë‘ í´ë˜ìŠ¤ë¡œë¶€í„° ìµœëŒ€í•œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ” ê²°ì • decision boundaryë¥¼ ì°¾ëŠ” ë¶„ë¥˜ê¸°ë¡œ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë™ì‹œì— í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒì„ ëª©í‘œ  
- ê°€ì¥ ì ì ˆí•œ decision boundaryë¥¼ ì°¾ê¸° ìœ„í•´ **hing loss**ë¥¼ loss functionìœ¼ë¡œ ì‚¬ìš©, A linear model with hinge loss is known as a ==support vector==(decision boundaryì—ì„œ ê°€ê¹Œìš´ sample) machine(SVM)
- SVMì€ **max-margin** or **large-margin**ì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•¨, train setê°€ _ì„ í˜•ì ìœ¼ë¡œ êµ¬ë¶„ë˜ëŠ” ê²½ìš°ì—ë§Œ ê°€ëŠ¥_ í•˜ë©° _ì´ìƒì¹˜ì— ë§¤ìš° ë¯¼ê°í•œ íŠ¹ì„±_ ì„ ê°€ì§€ê³  ìˆìŒ
	![](Pasted%20image%2020231213120711.png)
	![](Pasted%20image%2020231213114320.png)

#### Maximizing Margin for Non-Separable Data Points
- **ğœ‰**: slack(ì—¬ìœ ) variables, ì–´ëŠ ì •ë„ì˜ ì˜¤ì°¨ë¥¼ í—ˆìš©
- **Soft-margin SVM**
	![](Pasted%20image%2020231213121920.png)
	-  **ğ›¾**: maringê³¼ slack ì´í•© ê°„ì˜ trade-offë¥¼ ì¡°ì •í•˜ëŠ” hyperparameter, ì–¼ë§ˆë§Œí¼ì˜ slackì„ ê°€ì§€ê³  ì˜¤ì°¨ë¥¼ í—ˆìš©í•  ê²ƒì¸ì§€ ê²°
		- **ğ›¾** = 0: _w_ = 0ì„ ì–»ìŒ
		- **ğ›¾** = âˆ: hard-margin objective
- 
	![](Pasted%20image%2020231213121453.png)   
	![](Pasted%20image%2020231213124921.png) ![](Pasted%20image%2020231213125419.png)


### Additive Model
- ê¸°ë³¸ì ìœ¼ë¡œ ì„ í˜•íšŒê·€ ëª¨ë¸ì€ ì…ë ¥ ë³€ìˆ˜ì™€ ì¶œë ¥ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ê°€ ì„ í˜•ì´ë¼ëŠ” ê°•í•œ ê°€ì •ì„ í•„ìš”
- ë°˜ë©´ ==GAN==(Generalized Additive Model)ì€ **ì„ í˜•ì„± ê°€ì •ì„ ì™„í™”**í•˜ê³  **ê°ê°ì˜ ë…ë¦½ ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë¶€ë“œëŸ¬ìš´ ê³¡ì„ ìœ¼ë¡œ ëª¨ë¸ë§**, ì´ë¥¼ í†µí•´ **ë”ìš± ë³µì¡í•œ ë°ì´í„° íŒ¨í„´ì„ í¬ì°©**í•˜ëŠ” ë° ìœ ìš©
	- f(x) = s1(x1) + s2(x2) + ... + sp(xp)
	- f(x): ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ì¢…ì† ë³€ìˆ˜
	- x1, x2, ... xp: ë…ë¦½ ë³€ìˆ˜
	![](Pasted%20image%2020231213142829.png)
	![](Pasted%20image%2020231213142941.png)
	