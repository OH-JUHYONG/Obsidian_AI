---
tags:
  - linear-regression
  - weight
  - bias
  - loss-function
  - cost-function
  - score
  - residual
  - chain-rule
  - partial-derivatives
  - direct-solution
  - critical-point
  - feature-mapping
  - numerical-differentitation
  - gradient
  - gradient-descent
  - GD
  - eta
  - learning-rate
  - parameter
  - hyper-parameter
  - fixed-point
  - Î·
---
:  ì¢…ì† ë³€ìˆ˜ yì™€ í•˜ë‚˜ ì´ìƒì˜ ë…ë¦½ ë³€ìˆ˜ xì™€ì˜ ì„ í˜• ìƒê´€ ê´€ê³„ë¥¼ ëª¨ë¸ë§ í•˜ëŠ” ê¸°ë²•

- ==í•™ìŠµ ë°ì´í„°ì™€ ê°€ì¥ ì˜ ë§ëŠ” í•˜ë‚˜ì˜ ì§ì„ ì„ ì°¾ëŠ” ì¼==
- **ë‹¨ìˆœ ì„ í˜• íšŒê·€**: _y = Wx + b_
	- _W_ : ê°€ì¤‘ì¹˜(weight)
	- _b_ : í¸í–¥(bias)
	- _y_ : prediction, output

- What is Linear?
	- the target must be predicted as a linear function of the inputs
	![[Pasted image 20231027005245.png]]


#### Loss(objective, error) function
- ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì— ëŒ€í•œ ì˜¤ì°¨ì— ëŒ€í•œ ì‹
- **score**(ì˜ˆì¸¡ì) = ê°€ì¤‘ì¹˜ ë²¡í„° x feature map
	![[Pasted image 20231013223822.png]]
	- _y - t_: **residual**, ê°’ì´ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ
	- 1/2ì€ ë‚˜ì¤‘ì— ë¯¸ë¶„í• ë•Œ ê³„ì‚°ì´ í¸í•˜ë¼ê³  ë†“ëŠ” ê°’


#### Cost function
- loss function **averaged** over **all** training examples
	![[Pasted image 20231027005415.png]]

### Solving the optimization problem
- Partial derivatives(í¸ë¯¸ë¶„)ì´ ì¤‘ìš”
- **the chain rule**ì„ í¸ë¯¸ë¶„ì— ì ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŒ
	![[Pasted image 20231027013558.png]]
	![[Pasted image 20231027010110.png]]

#### Direct solution
- **Critical Point**(ì„ê³„ì ): ë„í•¨ìˆ˜ê°€ 0ì´ ë˜ëŠ” ì§€ì , ìµœì†Œê°’ì€ ë¶€ë¶„ ë„í•¨ìˆ˜ê°€ 0ì´ ë˜ëŠ” ì§€ì ì—ì„œ ë°œìƒ
- **Direct solution**: í¸ë¯¸ë¶„ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ê³  ë§¤ê°œë³€ìˆ˜ë¥¼ í‘¸ëŠ” ê²ƒ
	![[Pasted image 20231027012436.png]]

### Feature mapping
- Use a feature mapping that linear regression to learn nonlinear dependencies(ë¹„ì„ í˜• ì¢…ì†)
	![[Pasted image 20231027125527.png]]
	-  polynomial regressionì„ ì˜ˆë¡œ ë“¤ìŒ
	- ìœ„ ê³¼ì •(feature mapping)ì„ í†µí•´ linear gression ì²˜ëŸ¼ í•  ìˆ˜ ìˆìŒ


### Generalization
- ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì—ì„œ í•™ìŠµí•œ íŒ¨í„´ì„ **ìƒˆë¡œìš´ ë°ì´í„°ì— ì ìš©**í•  ìˆ˜ ìˆê²Œ generalizationí•˜ëŠ”ê²Œ ëª©í‘œ
	![[Pasted image 20231027125944.png]]

### Regularization
- ë°ì´í„°ë¥¼ íŠ¹ì • ë²”ìœ„ë¡œ ë³€í™˜
- ëª¨ë¸ ë³µì¡ë„ì— ëŒ€í•œ penaltyë¡œ ì •ê·œí™”ëŠ” overfittingì„ ì˜ˆë°©í•˜ê³  generalization ì„±ëŠ¥ì„ ë†’ì´ëŠ”ë° ë„ì›€ì„ ì¤Œì¤Œ

##### L2 Regularization
- ê¸°ì¡´ì˜ cost functionì— ê°€ì¤‘ì¹˜ì˜ ì œê³±ì„ í¬í•¨í•˜ì—¬ ë”í•¨ìœ¼ë¡œì¨ ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ í¬ì§€ ì•Šì€ ë°©í–¥ìœ¼ë¡œ í•™ìŠµ
- **weigth decay**ë¼ê³ ë„ í•¨
	![[Pasted image 20231027144418.png]]
	
	![[Pasted image 20231027175530.png]]
	![[Pasted image 20231027175544.png]]
	- í•™ìŠµì´ ì§„í–‰ë˜ë©´ì„œ ìµœì‹  wì— ëŒ€í•œ ì´ì „ iterationì˜ wì˜ ë¹„ì¤‘ì€ ì ì  ì‘ì•„ì§


##### L1 vs L2 Regularization
- **L1 norm**: íŠ¹ì • parameter ê°’ì„ 0ì„ ë§ì´ ë§Œë“œëŠ” ë°©í–¥
	- feature selectionì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— sparse modelì— ì í•©
	- convex(ì•„ë˜ë¡œ ë³¼ë¡) optimizationì— ìœ ìš© 
	- ë°‘ì— ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ì ì´ ìˆê¸° ë•Œë¬¸ì— gradient-base learningì—ëŠ” ì£¼ì˜ê°€ í•„ìš”
- **L2 norm**: ì „ì²´ê°€ ì‘ì•„ì§€ëŠ” ë°©í–¥
	- ê°ê°ì˜ ë²¡í„°ì— ëŒ€í•´ í•­ìƒ uniqueí•œ ê°’ì„ ë‚¼ ìˆ˜ ìˆìŒ
	![[Pasted image 20231027175837.png]]


---

### ìˆ˜ì¹˜ ë¯¸ë¶„(numerical differentiation)

- ê²½ì‚¬ë²•ì—ì„œëŠ” ê¸°ìš¸ê¸°(ê²½ì‚¬) ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ë°©í–¥ì„ ì •í•¨

##### ë¯¸ë¶„
- íŠ¹ì • ìˆœê°„ì˜ ë³€í™”ëŸ‰
	![[Pasted image 20231013224353.png]] 
	- ìœ„ ì‹ì˜ ì¢Œë³€ì€ _f(x)_ ì˜ _x_ ì— ëŒ€í•œ ë¯¸ë¶„(_x_ ì— ëŒ€í•œ _f(x)_ ì˜ ë³€í™”ëŸ‰)ì„ ë‚˜íƒ€ë‚¸ ê¸°í˜¸

```python
import numpy as np
import matplotlib.pylab as plt

# x: í•¨ìˆ˜ fì— ë„˜ê¸¸ ì¸ìˆ˜
def numerical_diff(f, x):
    h = 1e-4 # 0.0001
    return (f(x+h) - f(x-h)) / (2*h) # ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ (x+h)ì™€ xì˜ ì°¨ë¶„ì¸ ì „ë°© ì°¨ë¶„ì´ ì•„ë‹Œ (x+h)ì™€ (x-h)ã…‡ì¼ ë•Œì˜ í•¨ìˆ˜ fì˜ ì¤‘ì‹¬(ì¤‘ì•™) ì°¨ë¶„ì„ ì‚¬ìš©

def function_1(x):
    return 0.01*x**2 + 0.1*x 

def tangent_line(f, x):
    d = numerical_diff(f, x)
    print(d)
    y = f(x) - d*x
    return lambda t: d*t + y
     
x = np.arange(0.0, 20.0, 0.1) # 0ì—ì„œ 20ê¹Œì§€ 0.1 ê°„ê²©ìœ¼ë¡œ ë°°ì—´ xë¥¼ ë§Œë“¬
y = function_1(x)
plt.xlabel("x")
plt.ylabel("f(x)")

tf = tangent_line(function_1, 5)
y2 = tf(x)

plt.plot(x, y)
plt.plot(x, y2)
plt.show()

>>>
0.1999999999990898
```

	![[Pasted image 20231013224542.png]] 


##### í¸ë¯¸ë¶„(Partial derivatives)
- ë³€ìˆ˜ê°€ ì—¬ëŸ¿ì¸ í•¨ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„
- ë³€ìˆ˜ê°€ í•˜ë‚˜ì¸ ë¯¸ë¶„ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ íŠ¹ì • ì¥ì†Œì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ”ë° ì—¬ëŸ¬ ë³€ìˆ˜ ì¤‘ ëª©í‘œ ë³€ìˆ˜ í•˜ë‚˜ì— ì´ˆì ì„ ë§ì¶”ê³  ë‹¤ë¥¸ ë³€ìˆ˜ëŠ” ê°’ì„ ê³ ì •
	![[Pasted image 20231013224852.png]]
```python
# x0 = 3, x1 = 4 ì¼ ë•Œ, x0ì— ëŒ€í•œ í¸ë¯¸ë¶„ âˆ‚f/âˆ‚x0 ì„ êµ¬í•˜ë¼
def function_tmp1(x0):
    return x0*x0 + 4.0**2.0
numerical_diff(function_tmp1, 3.0)

>>>
6.00000000000378
```



##### ê¸°ìš¸ê¸°(gradient)
- ëª¨ë“  ë³€ìˆ˜ì˜ í¸ë¯¸ë¶„ì„ vectorë¡œ ì •ë¦¬í•œ ê²ƒ
```python
def numerical_gradient(f,x):
    h = 1e-4
    grad = np.zeros_like(x)
    
    # ë„˜íŒŒì´ ë°°ì—´ xì˜ ê° ì›ì†Œì— ëŒ€í•´ì„œ ìˆ˜ì¹˜ ë¯¸ë¶„
    for idx in range(x.size):
        tmp_val = x[idx]
        
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        x[idx] = tmp_val - h
        fxh2 = f(x)
        
        grad[idx] = (fxh1-fxh2)/(2*h)
        x[idx] = tmp_val
        
    return grad
    
print(numerical_gradient(function_2, np.array([3.0,4.0])))
print(numerical_gradient(function_2, np.array([0.0,2.0])))
print(numerical_gradient(function_2, np.array([3.0,0.0])))

>>>
[6. 8.]
[0. 4.]
[6. 0.]
```
	![[Pasted image 20231013225113.png]] 
	- ê¸°ìš¸ê¸°ëŠ” ê° ì§€ì ì—ì„œ ë‚®ì•„ì§€ëŠ” ë°©í–¥ì„ ê°€ë¦¬í‚´
	- ê¸°ìš¸ê¸°ê°€ ê°€ë¦¬í‚¤ëŠ” ìª½ì€ ê° ì¥ì†Œì—ì„œ í•¨ìˆ˜ì˜ ì¶œë ¥ ê°’ì„ ê°€ì¥ í¬ê²Œ ì¤„ì´ëŠ” ë°©í–¥


##### ê²½ì‚¬ë²• í•˜ê°•ë²•(gradient descent)
- í˜„ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ì¼ì • ê±°ë¦¬ë§Œí¼ ì´ë™, ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°€ê¸°ë¥¼ **ë°˜ë³µ**/í•¨ìˆ˜ì˜ ê°’ì„ ì ì°¨ ì¤„ì´ëŠ” ë°©ë²•   
	- GDëŠ” í›¨ì”¬ ë” ê´‘ë²”ìœ„í•œ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆìŒ
	- direct solutionë³´ë‹¤ êµ¬í˜„í•˜ê¸° ì‰¬ì›€
	- ê³ ì°¨ì›ì˜ íšŒê·€ì˜ ê²½ìš° direct solutionë³´ë‹¤ íš¨ìœ¨
- **gradient**: í•¨ìˆ˜ê°€ ì»¤ì§€ëŠ” ë°©í–¥
	![[Pasted image 20231027122834.png]]
	- **ğ½**: ì£¼ì–´ì§„ ìœ„ì¹˜ì—ì„œ í•¨ìˆ˜ì˜ ê°’ì´ ìµœëŒ€ê°€ ë˜ëŠ” ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” vector  
	- **âˆ‚ğ½/âˆ‚w** > 0: increasing w, increases ğ½
	- **âˆ‚ğ½/âˆ‚w** < 0: increasing w, decreases ğ½
	![[Pasted image 20231027123127.png]]

	![[Pasted image 20231013225354.png]]
	- **Î·**(eta): í•œ ë²ˆì˜ í•™ìŠµìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ í•™ìŠµí•´ì•¼ í• ì§€, ì¦‰ ë§¤ê°œë³€ìˆ˜ ê°’ì„ ì–¼ë§ˆë‚˜ ê°±ì‹ í•˜ëŠëƒë¥¼ ì •í•¨
		- ì‹ ê²½ë§ í•™ìŠµì—ì„œëŠ” **learning rate**(í•™ìŠµë¥ )ì´ë¼ê³  í•¨,
		- í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ê±°ì˜ ê°±ì‹ ì´ ë˜ì§€ ì•ŠìŒ
	
	- **parameter**
		- ëª¨ë¸ ë‚´ë¶€ì—ì„œ ê²°ì •ë˜ëŠ” ë³€ìˆ˜
		- ë°ì´í„°ë¡œë¶€í„° ê²°ì •
		- ì‚¬ìš©ìì— ì˜í•´ ì¡°ì •ë˜ì§€ ì•ŠìŒ
	- **hyper parameter**
		- ëª¨ë¸ë§í•  ë•Œ ì‚¬ìš©ìê°€ ì§ì ‘ ì„¸íŒ…í•´ ì£¼ëŠ” ê°’
		ex) learning rate, loss function, mini batch size...
	
	- **fixed points**(points where the iterate doesn't change)ë¥¼ ì°¾ëŠ”ë° ë„ì›€ì„ ì¤Œ
		- _divergence_ or _local optima_ ë¬¸ì œë¡œ ìµœì ì˜ í•´ë¥¼ ëª»ì°¾ì„ ìˆ˜ ìˆìŒ

```python
# f(x0, x1) = x0^2 + x1^2ì˜ ìµœì†Ÿê°’

import numpy as np
import matplotlib.pylab as plt

def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x # ì´ˆê¸°ê°’
    x_history = []

    # step num: ê²½ì‚¬ë²•ì— ë”°ë¥¸ ë°˜ë³µ íšŸìˆ˜
    for i in range(step_num):
        x_history.append( x.copy() )

        grad = numerical_gradient(f, x)
        x -= lr * grad
        
        # lr: learning rate, í•™ìŠµë¥ 

    return x, np.array(x_history)

def function_2(x):
    return x[0]**2 + x[1]**2

# ì´ˆê¸°ê°’ì„ (-3.0, 4.0)ìœ¼ë¡œ ì„¤ì •í•œ í›„ ê²½ì‚¬ë²• ì‚¬ìš©í•´ ìµœì†Ÿê°’ íƒìƒ‰
init_x = np.array([-3.0, 4.0])    

# lrì€ ë¯¸ë¦¬ ì •í•´ì•¼ í•¨, ì ì ˆí•œ ê°’ì„ ì„¤ì •í•˜ëŠ”ê²Œ í•µì‹¬
lr = 0.1
step_num = 20
x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)

plt.plot( [-5, 5], [0,0], '--b')
plt.plot( [0,0], [-5, 5], '--b')
plt.plot(x_history[:,0], x_history[:,1], 'o')

plt.xlim(-3.5, 3.5)
plt.ylim(-4.5, 4.5)
plt.xlabel("X0")
plt.ylabel("X1")
plt.show()

>>>
array([-6.11110793e-10, 8.14814391e-10]) ìœ¼ë¡œ ê±°ì˜ (0, 0)ì— ê°€ê¹Œìš´ ê²°ê³¼ë¡œ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ
```
	![[Pasted image 20231014140154.png]] 
